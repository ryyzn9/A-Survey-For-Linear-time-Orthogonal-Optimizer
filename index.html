<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>AuON: A Survey For Linear-time Orthogonal Optimizer — Dipan Maity</title>

  <!-- MathJax for LaTeX rendering -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>

 <style>
  :root {
    --bg:#ffffff; 
    --card:#f9fafb; 
    --muted:#374151; 
    --accent:#0d47a1; 
    --glass:rgba(0,0,0,0.03);
    --content-width:900px;
    color-scheme: light;
  }

  html, body {
    height:100%;
    margin:0;
    background:linear-gradient(180deg,#ffffff 0%, #f5f5f5 100%);
    font-family:Inter,ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,"Helvetica Neue",Arial;
    color:#111827;
  }

  .container {
    max-width:var(--content-width);
    margin:36px auto;
    padding:28px;
    background:var(--card);
    border-radius:12px;
    box-shadow:0 4px 20px rgba(0,0,0,0.08);
  }

  header {
    display:flex;
    gap:16px;
    align-items:center;
    margin-bottom:18px;
  }

  .title {
    font-size:22px;
    font-weight:700;
    color:#111827;
  }

  .byline {
    color:var(--muted);
    font-size:13px;
  }

  nav { margin-left:auto }
  nav a {
    color:var(--accent);
    text-decoration:none;
    font-weight:600;
  }
  nav a:hover { text-decoration:underline; }

  .meta { display:flex; gap:12px; align-items:center; margin:8px 0 18px }

  .section { margin-top:22px }

  h2 { color:#111827; margin-bottom:8px }
  p { color:#1f2937; line-height:1.7 }

  .abstract {
    background:var(--glass);
    padding:14px;
    border-radius:8px;
    border:1px solid rgba(0,0,0,0.06);
  }

  .figure {
    margin:14px 0;
    text-align:center;
  }
  .figure img {
    max-width:100%;
    height:auto;
    border-radius:8px;
    border:1px solid rgba(0,0,0,0.08);
  }

  table {
    width:100%;
    border-collapse:collapse;
    margin-top:12px;
    color:#111827;
    font-size:0.95rem;
  }
  table th, table td {
    padding:8px 10px;
    border:1px solid rgba(0,0,0,0.1);
    text-align:left;
  }
  table th {
    background:#f1f5f9;
    font-weight:600;
  }
  table tr:nth-child(even) {
    background:#f9fafb;
  }

  .code {
    background:#f9fafb;
    padding:12px;
    border-radius:8px;
    border:1px solid rgba(0,0,0,0.08);
    overflow:auto;
    color:#111827;
    font-family:"Fira Code", monospace;
    font-size:0.95em;
  }

  footer {
    margin-top:30px;
    color:var(--muted);
    font-size:13px;
    text-align: center;
  }

  /* Blockquotes for quotes */
  blockquote {
    margin: 1.5rem 2rem;
    padding: 0.8rem 1rem;
    border-left: 4px solid var(--accent);
    background: #f9fafb;
    font-size: 1.05em;
    color: #374151;
    font-style: italic;
  }

  /* Responsive */
  @media (max-width:640px){
    .container{margin:12px;padding:16px} 
    .title{font-size:18px}
  }
</style>
</head>
<body>
  <div class="container">
    <header>
      <div>
        <div class="title">AuON: A Survey For Linear-time Orthogonal Optimizer</div>
        <div class="byline">Dipan Maity — <a href="mailto:dipanai.xyz@gmail.com" style="color:var(--accent)">dipanai.xyz@gmail.com</a></div>
      </div>
      <nav><a href="#references">References</a></nav>
    </header>

    <section class="abstract">
      <strong>Abstract</strong>
      <p>Orthogonal gradient updates have emerged as a promising direction in optimization
for machine learning.However, traditional approaches such as SVD/QR decompo-
sition incur prohibitive computational costs of O(n3) and underperform compared
to well-tuned SGD with momentum,since momentum is applied only after strict
orthogonalization. Recent advances, such as Muon, improve efficiency by applying
momentum before orthogonalization and producing semi-orthogonal matrices via
Newton–Schulz iterations, reducing complexity to O(n2). Nevertheless, quadratic
costs remain a bottleneck.
In this work, we study the semi-orthogonal properties of momentum-based up-
dates and develop a method to bound momentum updates under a spectral-norm
trust region, preserving directional information without requiring explicit semi-
orthogonalization.
We propose AuON (Alternative Unit-norm momentum updates by Normalized
nonlinear scaling), a linear-time optimizer that achieves strong performance without
constructing semi-orthogonal matrices, while still preserving structural alignment
and reconditioning ill-posed updates. Our approach combines hyperbolic-cosine
RMS scaling transformations with normalization, demonstrating both effectiveness
and computational efficiency compared to Newton–Schulz methods. We further
introduce a hybrid variant (Hybrid-AuON) that applies a single Newton–Schulz
iteration. Experiments across vision and language benchmarks show that AuON
and its hybrid variant achieve performance comparable to state-of-the-art optimiz-
ers. Code: <a href="https://github.com/ryyzn9/AuON" target="_blank">github.com/ryyzn9/AuON</a>.</p>
    </section>

    <section class="section" id="introduction">
      <h2>Introduction</h2>

    <blockquote>
    <em>
      “If you want to achieve extraordinary progress in AI, you should enhance the optimizer, 
      as it fundamentally determines how models learn.”
    </em>
  </blockquote>
  <p>
    Optimization in deep neural networks remains a central challenge, particularly due to 
    the ill-conditioning of gradient and momentum updates. Empirically, these updates often 
    exhibit a high condition number, with most of the energy concentrated in a few dominant 
    directions. In practical terms, the update vectors are nearly low-rank: a handful of 
    directions dictate the optimization trajectory while many potentially informative 
    directions may be suppressed. This imbalance reminds us of a squashed ball that can only 
    roll efficiently along a single axis, ignoring other pathways that may be equally 
    important for generalization and representation learning. One solution is to make all 
    update directions unit length; recent work has proposed orthogonalization of gradients 
    and momentum updates to achieve this property.
  </p>

  <div class="figure">
    <img src="assets/all2.png" alt="Visualization of Newton–Schulz process">
    <div style="color:#9aa4b2;font-size:14px;margin-top:6px">
      Visualization of the Newton–Schulz process (0.5) over 5 iterations, compared with AuON 
      and Hybrid-AuON. The heatmaps (top) show progressive orthogonalization, with 
      \( M M^\top \) converging from a scattered structure (Step 0) to an identity-like diagonal (Step 5). 
      The singular value plot (bottom) illustrates rapid convergence toward \(1.0\), confirming orthogonalization.
    </div>
  </div>

  <p>
    By orthogonalizing an update matrix, we effectively discard the scaling information encoded 
    in the singular values and modify each direction to enforce perpendicularity, redistributing 
    the update length into unit vectors along different directions. In this sense, the resulting 
    update behaves as a <em>unit-norm</em> update in the spectral domain, emphasizing the geometric 
    structure of the optimization landscape rather than the raw gradient magnitudes. In simple terms, 
    orthogonalization amplifies “rare directions” with small magnitude in the update but which are 
    nevertheless important for learning. This perspective highlights how orthogonalization can 
    prioritize exploration across all relevant directions, mitigating the dominance of a few 
    high-energy components and facilitating more balanced learning dynamics.
  </p>

  <p>
    Tuddenham et al. (2022) proposed an approach for neural network optimization in which the 
    gradient is first orthogonalized via singular value decomposition (SVD), followed by the 
    application of momentum, and then the resulting momentum term is used as the update. They 
    refer to this method as Orthogonal-SGDM. In their experiments, they observed that even in 
    the best-performing configuration, Orthogonal-SGDM was outperformed by a well-tuned standard 
    SGD with momentum. This is because applying momentum after strict orthogonalization damages 
    the momentum mechanism: orthogonalizing gradients before accumulation prevents momentum 
    from effectively reducing variance and maintaining beneficial directional information. 
    Moreover, strict orthogonality erases singular-value magnitudes and over-constrains the step, 
    collapsing its singular-value structure to an isometry. In effect, the update becomes a 
    spectral-norm–constrained move that discards useful magnitude information, wiping out 
    correlations between update directions. Making all updates unit length may also increase 
    harmful alignment effects. 
  </p>

  <p>
    Recent advances, such as Muon (Jordan, 2024), improve efficiency and performance by producing 
    a semi-orthogonal matrix using Newton–Schulz iterations rather than a full orthogonal matrix 
    using SVD, and by reordering the momentum update to occur before semi-orthogonalization. 
    This reduces complexity to \(\mathcal{O}(n^2)\), but quadratic costs still remain a bottleneck.
  </p>

  <p>
    In this paper, we focus on developing an alternative approach to bound updates with high 
    condition numbers under a unit-norm constraint. Our goal is to achieve strong performance 
    with \(\mathcal{O}(n)\) time complexity, without compromising efficiency or speed. 
    Empirically, we find that normalization followed by a hyperbolic-cosine scaling transformation 
    yields promising results. 
  </p>
    </section>

    <section id="preliminaries">
  <h2>Preliminaries</h2>

  <h3>Orthogonalization</h3>
  <p>
    By orthogonalizing an update matrix 
    \(G \in \mathbb{R}^{m \times n}\) 
    with singular value decomposition
  </p>
  <div>
    \[
    G = U \Sigma V^{\top},
    \]
  </div>
  <p>
    the update is replaced by its orthogonal polar factor:
  </p>
  <div>
    \[
    Q := U V^{\top}.
    \]
  </div>
  <p>
    This satisfies
  </p>
  <div>
    \[
    Q^{\top}Q = I_n \quad \text{when } m \geq n,
    \qquad
    QQ^{\top} = I_m \quad \text{when } m \leq n,
    \]
  </div>
  <p>
    thereby discarding the scaling information carried by the singular values 
    \(\Sigma\) while preserving the directional subspaces encoded by the 
    left and right singular vectors \(U\) and \(V\).
  </p>
  <p>
    In this sense, the resulting update behaves as unit-norm in the spectral domain—
  </p>
  <div>
    \[
    \|Q\|_{2} = 1
    \]
  </div>
  <p>
    with a flat singular spectrum—emphasizing the geometric structure of the 
    optimization landscape rather than the raw gradient magnitudes.
  </p>
  <p>
    Intuitively, this equalizes per-direction gain: directions that originally 
    had small singular values (“rare directions”) are relatively amplified while 
    dominant directions are relatively attenuated, promoting exploration across 
    all relevant directions and mitigating the dominance of a few high-energy modes.
  </p>
  <p>
    In practice, orientation and step size can be decoupled by using
  </p>
  <div>
    \[
    \alpha Q,
    \qquad 
    \alpha = \frac{\|G\|_{F}}{\sqrt{\mathrm{rank}(G)}},
    \]
  </div>
  <p>
    so that scale is controlled externally while orthogonalization enforces 
    well-conditioned, balanced updates—yielding more stable and equitable 
    learning dynamics compared to conventional gradient-descent steps.
  </p>

  <h3>Semi-orthogonalization</h3>
  <p>
    Given \(G \in \mathbb{R}^{m \times n}\) with singular value decomposition
  </p>
  <div>
    \[
    G = U \Sigma V^{\top},
    \]
  </div>
  <p>
    strict orthogonalization replaces \(G\) by its polar/Stiefel projection
  </p>
  <div>
    \[
    Q := U V^{\top},
    \]
  </div>
  <p>
    collapsing the singular spectrum to \(\sigma_i(Q) = 1\) on the update subspace and 
    making \(Q\) an isometry with
  </p>
  <div>
    \[
    \|Q\|_{2} = 1, 
    \qquad 
    Q^{\top} Q = I_n \; \; (\text{or } \; QQ^{\top} = I_m),
    \]
  </div>
  <p>
    i.e., the Frobenius-nearest semi-orthogonal matrix that removes the amplitude 
    information in \(\Sigma\).
  </p>
  <p>
    In the singular basis,
  </p>
  <div>
    \[
    G^{\top} G = V \Sigma^{2} V^{\top}
    \]
  </div>
  <p>
    becomes
  </p>
  <div>
    \[
    Q^{\top} Q = I,
    \qquad
    QQ^{\top} = U I U^{\top} = \Pi_{\mathrm{col}(G)}.
    \]
  </div>
  <p>
    Geometrically, for Muon’s RMS-to-RMS operator norm, we have
  </p>
  <div>
    \[
    Q \in \arg\max_{\|X\|_{\mathrm{RMS}\to\mathrm{RMS}} \leq 1} \langle X, G \rangle,
    \]
  </div>
  <p>
    which is the linear minimization oracle (LMO) of a conditional-gradient step. 
    Hence, the singular values are flattened; by contrast, on the standard 
    spectral-norm ball, the LMO yields the rank-1 solution \(u_{1} v_{1}^{\top}\).
  </p>
  <p>
    To avoid overconstraint, semi-orthogonal schemes such as Muon orthogonalize 
    only the momentum \(M_t\) to
  </p>
  <div>
    \[
    Q_t = \mathrm{polar}(M_t),
    \]
  </div>
  <p>
    and decouple scale via an RMS-to-RMS factor \(\alpha\), giving
  </p>
  <div>
    \[
    W_{t+1} = (1 - \eta_t \lambda) W_t + \eta_t \, \alpha \, Q_t.
    \]
  </div>
  <p>
    In practice, \(Q_t Q_t^{\top}\) is computed efficiently via a low-order 
    Newton–Schulz iteration, and \(\alpha\) is chosen to match update RMS across shapes. 
    Semi-orthogonalization stabilizes training by bounding spectral energy, equalizing 
    directional gains, preventing overshoot, and enabling larger learning rates 
    by decoupling orientation from scale.
  </p>

  <h3>Orthogonalized Momentum as a Spectral Trust-Region Method</h3>
  <p>
    Recent advances demonstrate that orthogonalized momentum in deep learning optimizers, 
    particularly the Muon optimizer, admits a principled interpretation as the solution 
    to a non-Euclidean trust-region subproblem under the spectral norm constraint. 
    The core update rule can be formulated as
  </p>
  <div>
    \[
    X_{k+1} = X_k - \eta O_k,
    \quad 
    O_k = \mathrm{Orth}\!\left(\nabla F(X_k)\right),
    \]
  </div>
  <p>
    where \(\mathrm{Orth}(\cdot)\) denotes the SVD-based orthogonalization operator that computes
  </p>
  <div>
    \[
    M = U \Sigma V^\top 
    \quad \Longrightarrow \quad 
    \mathrm{Orth}(M) = U V^\top,
    \]
  </div>
  <p>
    yielding the steepest descent direction under the spectral norm metric.
  </p>

  <p><strong>Momentum Integration.</strong></p>
  <p>
    The momentum component follows the exponential moving average:
  </p>
  <div>
    \[
    m_{k+1} = (1-\alpha)m_k + \alpha \, g(x_k;\xi_k),
    \]
  </div>
  <p>
    where \(g(x_k;\xi_k)\) represents an unbiased stochastic gradient estimate. 
    The orthogonalized update then solves the trust-region subproblem
  </p>
  <div>
    \[
    x_{k+1} 
    = \arg\min_{x} 
    \Big\{ \langle \mathrm{Orth}(m_{k+1}), \, x \rangle 
    : \, \|x - x_k\|_2 \leq \eta \Big\}.
    \]
  </div>
  <p>
    This formulation explicitly constrains parameter updates within a trust region 
    while ensuring the search direction maintains unit spectral norm.
  </p>

  <p><strong>Theoretical Advantages.</strong></p>
  <p>
    The orthogonalization-first approach provides superior variance reduction compared 
    to alternative momentum–orthogonalization orderings. By applying orthogonalization 
    to the momentum vector before the parameter update, the method preserves accumulated 
    directional information while eliminating scale-dependent instabilities. This design 
    has theoretical convergence guarantees and empirical improvements in training stability 
    across diverse architectures.
  </p>
</section>

      <section id="methods">
  <h2>Methods</h2>
  <p>
    We hypothesize that forcing all update directions to unit length can be problematic, 
    as not all directions contribute equally to optimization progress—some may be harmful 
    (having a negative impact) or irrelevant to loss reduction. Our goal is to develop 
    an alternative method that removes the harmful directions or alignments and preserves 
    the beneficial properties of near semi-orthogonalization while selectively scaling 
    directions under unit-norm: decrease the scales of rare update directions relative 
    to dominant ones and keep them all under a unit-norm trust region. One solution is 
    to apply a temperature-scaled softmax update matrix, followed by L2-renormalization, 
    to bound the step under a trust region. But computing softmax may be problematic as 
    it introduces computational bottlenecks, and it does not preserve the semi-orthogonal 
    property that is needed for an optimizer like Muon.
  </p>
  <p>
    We empirically find that normalization with hyperbolic functions (\(\cosh\)) helps us 
    achieve a spectral-norm trust region and helps preserve near semi-orthogonal properties, 
    yielding more stable and equitable learning dynamics compared to conventional 
    gradient-descent steps. It stabilizes training by bounding spectral energy into a 
    unit vector and equalizing directional gains, preventing overshoot along sharp curvature, 
    reducing oscillations, and enabling larger learning rates by decoupling orientation 
    from scale.
  </p>

  <h3>Nonlinear reshaping via hyperbolic cosine RMS scaling</h3>
  <p>
    Our main goal is to keep all the updated directions under unit spectral norm and 
    remove the harmful directions. We empirically find that dividing the update matrix 
    by the RMS magnitude of \(\cosh(\cdot)\) bounds dominant update directions and 
    preserves near semi-orthogonal-like properties.
  </p>

  <div>
    \[
    X = \frac{G}{\|G\| + 10^{-7}}, \quad 
    \text{update} = X, \quad 
    x = \cosh(\text{update}), \quad 
    \text{rms} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} x_i^2}, \quad 
    G = \frac{\text{update}}{\text{rms} + 10^{-8}}
    \]
  </div>

  <p>
    where
  </p>
  <div>
    \[
    \cosh(z) = \frac{e^z + e^{-z}}{2}.
    \]
  </div>

  <p>
    For large \(|z|\), \(\cosh(z)\) grows exponentially, while for small \(z\),
  </p>
  <div>
    \[
    \cosh(z) \approx 1 + \frac{z^2}{2}.
    \]
  </div>
  <p>
    Thus, \(\cosh\) magnifies meaningful deviations while remaining symmetric and smooth. 
    This encourages a spread of activations (diversity) without enforcing strict orthogonality.
  </p>

  <p><strong>Effect of the Hyperbolic Cosine RMS Magnitude.</strong></p>
  <p>
    Define \(\mathrm{rms} := \|\cosh(\mathrm{update})\|_{F}/\sqrt{N}\). 
    Because \(\cosh\) is even and rapidly increasing in \(|x|\), heavy tails inflate 
    \(\mathrm{rms}\), which reduces the overall step size when forming 
    \(U := \mathrm{update}/(\mathrm{rms}+10^{-8})\). 
    Crucially, \(\cosh\) is not applied to the propagated vector: \(U\) is a uniform rescaling 
    of \(\mathrm{update}\), so the signs and all relative component ratios are preserved. 
    This yields scale invariance with tail-aware damping, without per-coordinate reweighting.
  </p>

  <p><strong>Layman’s terms.</strong> First, fix the raw step’s size; then gauge how “spiky” it is 
  using \(\cosh\); finally, shrink the whole step more if it looks spiky. The direction and 
  proportions stay the same.</p>

  <p><strong>Near–Semi–Orthogonality.</strong></p>
  <p><em>Exact orthogonality.</em> A matrix \(W \in \mathbb{R}^{m \times n}\) is orthogonal 
  (semi-orthogonal if \(m \neq n\)) when:</p>
  <div>
    \[
    W^\top W = I_n \quad \text{or} \quad W W^\top = I_m.
    \]
  </div>

  <p><em>Method equations.</em> Let \(G \in \mathbb{R}^{m \times n}\), \(N = mn\). Define:</p>
  <div>
    \[
    \mathrm{update} := \frac{G}{\|G\|_{F} + 10^{-7}}, \quad
    \mathrm{rms} := \frac{\|\cosh(\mathrm{update})\|_{F}}{\sqrt{N}}, \quad
    U := \frac{\mathrm{update}}{\mathrm{rms} + 10^{-8}}.
    \]
  </div>

  <p><strong>Immediate implications:</strong></p>
  <ul>
    <li><em>Scale invariance.</em> For any \(c>0\), replacing \(G\) by \(cG\) leaves 
    \(\mathrm{update}\) (and thus \(U\)) unchanged up to \(\varepsilon\)-terms.</li>
    <li><em>Tail-aware global scaling.</em> Heavy tails inflate \(\mathrm{rms}\), reducing 
    the global magnitude of \(U\).</li>
    <li><em>No per-component reweighting.</em> \(U\) is a uniform rescaling of 
    \(\mathrm{update}\); ratios are preserved.</li>
  </ul>

  <p><strong>Norms and “balanced sphere.”</strong></p>
  <div>
    \[
    \|\mathrm{update}\|_{F} \approx 1, \quad
    \|U\|_{F} \approx \frac{1}{\mathrm{rms} + 10^{-8}}, \quad
    \mathrm{RMS}(U) \approx \frac{1}{\sqrt{N}(\mathrm{rms}+10^{-8})}.
    \]
  </div>
  <p>
    Thus, there is no unit-L2 or unit-RMS constraint; the overall step length decreases as 
    the tail-sensitive scalar \(\mathrm{rms}\) increases.
  </p>

  <p><strong>Relation to near semi-orthogonality.</strong></p>
  <div>
    \[
    M := U^\top U, \quad \operatorname{tr}(M) = \|U\|_F^2, \quad 
    \alpha := \frac{1}{n}\operatorname{tr}(M).
    \]
  </div>
  <p>
    Off-diagonal correlations \(M_{ij} = \langle U_{:i}, U_{:j} \rangle\) are not explicitly 
    zeroed, so the mapping promotes approximate isotropy rather than exact semi-orthogonality.
  </p>
  <ul>
    <li><em>Cross-correlations.</em> Off-diagonals remain scaled copies of 
    \(\mathrm{update}^\top \mathrm{update}\).</li>
    <li><em>Isotropy.</em> Alone, this does not drive \(M\) toward \(\alpha I_n\). 
    Additional correlation-reducing steps may be required (e.g., whitening, penalties).</li>
  </ul>

  <p><strong>Practical implication.</strong> The update is scale-invariant and tail-aware: 
  heavy tails trigger stronger shrinkage via \(\mathrm{rms}\), helping prevent blow-ups while 
  preserving direction and internal proportions. When approximate isotropy is desired, pair 
  with a lightweight correlation-suppressing operation.</p>

  <h3>Hybrid Approach</h3>
  <div class="figure">
    <img src="assets/auon2.png" alt="Comparison of computation efficiency">
    <div>Comparison of computation efficiency of different methods on (n×n) random matrices.</div>
  </div>
  <p>
    The hybrid approach includes only one iteration of Newton–Schulz and nonlinear reshaping 
    via hyperbolic cosine RMS scaling. This improves performance with only one iteration.
  </p>

  <div>
    \[
    A = X X^\top, \quad
    B = bA + cA^2, \quad
    X \leftarrow aX + BX
    \]
  </div>
  <div>
    \[
    G_{\text{new}} = \frac{\text{update}}{\mathrm{rms} + \delta}, \quad 
    \text{with } \text{update} = X, \quad 
    \mathrm{rms} = \frac{1}{N} \| \cosh(X) \|_F^2.
    \]
  </div>

  <p>
    This achieves near semi-orthogonality and bounds the updates under a spectral-norm 
    trust region with far less computation than 5 iterations of Newton–Schulz in Muon, 
    while maintaining competitive performance compared to AdamW and Muon.
  </p>
</section>

      
    <section id="experiments">
  <h2>Experiments</h2>

  <h3>Language Modeling</h3>
  <p>
    We evaluate our approach using a 4×L4 GPU on the <strong>SmolLM-Corpus</strong> dataset 
    (500k tokens). The underlying model is a <strong>nanoGPT</strong> with 
    FlashAttention-2, rotary position embeddings (RoPE), RMSNorm, and SwiGLU activations. 
    For the <em>Small configuration</em>, we use a hidden size of 512, 6 layers, 
    8 attention heads, and a feed-forward dimension of 1536. 
    Training is conducted for 6000 steps with a global batch size of 128.
  </p>
  <p>
    We compare <strong>AuON</strong>, AdamW, Hybrid-AuON, and MuON under similar conditions, 
    with learning rates tuned separately: 
    \(\eta_{\text{adamw}} = 0.003\), \(\eta_{\text{auon}} = 0.055\), 
    \(\eta_{\text{muon}} = 0.01\).
  </p>

  <table>
    <caption style="caption-side:top;text-align:left;font-weight:bold;margin-bottom:6px">
      Training Results on Tiny (Run 1). All optimizers trained under identical settings.
    </caption>
    <thead>
      <tr>
        <th>Optimizer</th>
        <th>Total Params</th>
        <th>Opt. Params</th>
        <th>Time (s)</th>
        <th>Loss</th>
        <th>Acc</th>
        <th>PPL</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>AuON</td>
        <td>40,901,120</td>
        <td>15,728,640</td>
        <td>1919.2</td>
        <td>0.4305</td>
        <td>0.8667</td>
        <td>1.54</td>
      </tr>
      <tr>
        <td>AdamW</td>
        <td>40,901,120</td>
        <td>40,901,120</td>
        <td>1918.9</td>
        <td>0.0686</td>
        <td>0.9846</td>
        <td>1.07</td>
      </tr>
      <tr>
        <td>Hybrid-AuON</td>
        <td>40,901,120</td>
        <td>15,728,640</td>
        <td>2285.4</td>
        <td>0.0422</td>
        <td>0.9908</td>
        <td>1.04</td>
      </tr>
      <tr>
        <td>MuON</td>
        <td>40,901,120</td>
        <td>15,728,640</td>
        <td>2303.6</td>
        <td>0.0375</td>
        <td>0.9919</td>
        <td>1.04</td>
      </tr>
    </tbody>
  </table>

  <h3>Vision Task (CIFAR-10)</h3>
  <p>
    We evaluated AdamW and the proposed AuON optimizer on the CIFAR-10 dataset 
    under a reduced-scale training protocol. The dataset was split into 
    <strong>15,000 training</strong>, <strong>1,500 validation</strong>, and 
    <strong>5,000 test</strong> samples. A batch size of 32 was used initially.
  </p>
  <p>
    <strong>Training configuration:</strong> 100 epochs, 
    learning rate = \(1 \times 10^{-3}\), Muon LR = 0.055, 
    weight decay = \(1 \times 10^{-4}\), 
    momentum \((\beta_{1}, \beta_{2}) = (0.9, 0.99)\). 
    The network contained 19.90M parameters.
  </p>

  <p><strong>Results (Test Accuracy):</strong></p>
  <ul>
    <li>AdamW: 76.0%</li>
    <li>AuON: 73.3%</li>
    <li>\(\Delta = -2.7\) percentage points</li>
  </ul>

  <div class="figure">
    <img src="assets/scr.png" alt="Training and validation curves on CIFAR-10">
    <div>
      Training and validation curves on CIFAR-10 with AdamW and AuON optimizers.  
      (Left) AdamW's loss decreases steadily. (Right) Accuracy shows AdamW converging to ~76% 
      while AuON plateaus around 73%.
    </div>
  </div>

  <p>
    Increasing the batch size from 32 to 128 improves performance:
  </p>
  <ul>
    <li>AdamW: 76.32%</li>
    <li>AuON: 76.22%</li>
    <li>\(\Delta = -0.10\) percentage points</li>
  </ul>
</section>

<section id="conclusion">
  <h2>Conclusion</h2>
  <p>
    In this paper, we developed a linear-time optimizer that enforces a unit spectral norm 
    and leverages semi-orthogonal properties to stabilize training without requiring 
    full semi-orthogonalization. Our experiments suggest that AuON and its hybrid variant 
    may suffer from exploding attention logits on large-parameter models. 
    Techniques such as QK-clipping may help mitigate this issue.
  </p>
  <p>
    Empirically, scaling model parameters increased AuON’s accuracy to 92%, with further 
    improvements on downstream tasks. As future work, we plan to evaluate AuON on larger-scale 
    setups (e.g., NanoGPT on H100 GPUs) to assess its performance in practical training scenarios.
  </p>
</section>


    <section id="references">
  <h2>References</h2>
  <ol>
    <li>
      Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, 
      Laker Newhouse, Jeremy Bernstein.  
      <em>Muon: An optimizer for hidden layers in neural networks</em>. 2024.  
      <a href="https://kellerjordan.github.io/posts/muon/" target="_blank">Link</a>
    </li>

    <li>
      Mark Tuddenham, Adam Prügel-Bennett, Jonathan Hare.  
      <em>Orthogonalising gradients to speed up neural network optimisation</em>. arXiv:2202.07052, 2022.  
      <a href="https://arxiv.org/abs/2202.07052" target="_blank">arXiv</a>
    </li>

    <li>
      Minxin Zhang, Yuxuan Liu, Hayden Schaeffer.  
      <em>AdaGrad Meets Muon: Adaptive Stepsizes for Orthogonal Updates</em>. arXiv:2509.02981, 2025.  
      <a href="https://arxiv.org/abs/2509.02981" target="_blank">arXiv</a>
    </li>

    <li>
      Dmitry Kovalev.  
      <em>Understanding Gradient Orthogonalization for Deep Learning via Non-Euclidean Trust-Region Optimization</em>. arXiv:2503.12645, 2025.  
      <a href="https://arxiv.org/abs/2503.12645" target="_blank">arXiv</a>
    </li>

    <li>
      Mark Peletier, André Schlichting.  
      <em>Cosh gradient systems and tilting</em>. Nonlinear Analysis, 231:113094, 2022/2023.  
      <a href="http://dx.doi.org/10.1016/j.na.2022.113094" target="_blank">DOI</a>
    </li>

    <li>
      Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, Yanru Chen, Huabin Zheng, Yibo Liu, Shaowei Liu, Bohong Yin, Weiran He, Han Zhu, Yuzhi Wang, Jianzhou Wang, Mengnan Dong, Zheng Zhang, Yongsheng Kang, Hao Zhang, Xinran Xu, Yutao Zhang, Yuxin Wu, Xinyu Zhou, Zhilin Yang.  
      <em>Muon is Scalable for LLM Training</em>. arXiv:2502.16982, 2025.  
      <a href="https://arxiv.org/abs/2502.16982" target="_blank">arXiv</a>
    </li>

    <li>
      Nicholas Higham.  
      <em>Matrix Nearness Problems and Applications</em>. 2000.  
      <a href="https://www.researchgate.net/publication/2640282_Matrix_Nearness_Problems_and_Applications" target="_blank">Link</a>
    </li>

    <li>
      James R. Lee.  
      <em>Von Neumann's Inequality and Unitarily-Invariant Norms</em>. Lecture notes, CSE599I, 2021.  
      <a href="https://example.edu/cse599i/vonneumann-notes.pdf" target="_blank">Link</a>
    </li>

    <li>
      Vuk Rosić, Claude.  
      <em>Muon vs AdamW: Learning Rate And Scaling Small LLMs</em>. GitHub, 2025.  
      <a href="https://github.com/vukrosic/muon-optimizer-research" target="_blank">Repo</a>
    </li>

    <li>
      Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, Leandro von Werra.  
      <em>SmolLM-Corpus</em>. Hugging Face, 2024.  
      <a href="https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus" target="_blank">Dataset</a>
    </li>

    <li>
      Ilya Loshchilov, Frank Hutter.  
      <em>Decoupled Weight Decay Regularization</em>. arXiv:1711.05101, 2019.  
      <a href="https://arxiv.org/abs/1711.05101" target="_blank">arXiv</a>
    </li>

    <li>
      Noam Shazeer.  
      <em>GLU Variants Improve Transformer</em>. arXiv:2002.05202, 2020.  
      <a href="https://arxiv.org/abs/2002.05202" target="_blank">arXiv</a>
    </li>

    <li>
      Shiyu Huang, Yuxin Su, Xuezhe Ma, Noah A. Smith.  
      <em>Root Mean Square Layer Normalization</em>. arXiv:1910.07467, 2019.  
      <a href="https://arxiv.org/abs/1910.07467" target="_blank">arXiv</a>
    </li>

    <li>
      Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu.  
      <em>RoFormer: Enhanced Transformer with Rotary Position Embedding</em>. arXiv:2104.09864, 2023.  
      <a href="https://arxiv.org/abs/2104.09864" target="_blank">arXiv</a>
    </li>

    <li>
      Andrej Karpathy.  
      <em>NanoGPT</em>. GitHub repository, 2022.  
      <a href="https://github.com/karpathy/nanoGPT" target="_blank">Repo</a>
    </li>

    <li>
      Tri Dao.  
      <em>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</em>. arXiv:2307.08691, 2023.  
      <a href="https://arxiv.org/abs/2307.08691" target="_blank">arXiv</a>
    </li>

    <li>
      Kimi Team et al.  
      <em>Kimi K2: Open Agentic Intelligence</em>. arXiv:2507.20534, 2025.  
      <a href="https://arxiv.org/abs/2507.20534" target="_blank">arXiv</a>
    </li>
  </ol>
</section>

    <footer>
      <div>To host this page on GitHub Pages: place this file as <code>index.html</code> in your repo (or in <code>docs/</code>), add the image files into <code>assets/</code>, push to GitHub, then enable Pages in repository settings (or use <code>username.github.io</code> repo). See the short instructions below.</div>
    </footer>
  </div>

  <!-- Minimal helper script to enable smooth scroll for nav -->
  <script>
    document.querySelectorAll('a[href^="#"]').forEach(a=>a.addEventListener('click',e=>{e.preventDefault();document.querySelector(a.getAttribute('href')).scrollIntoView({behavior:'smooth'});}));
  </script>
</body>
</html>
